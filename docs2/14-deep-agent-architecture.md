# ë”¥ ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜ ê°œì„  ê³„íš

ì´ ë¬¸ì„œëŠ” HolmesGPTì˜ í˜„ì¬ ë‹¨ì¼ ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜ë¥¼ ë¶„ì„í•˜ê³ , ë©€í‹° ì—ì´ì „íŠ¸ ì „í™˜, LangChain í†µí•©, ëŸ°ë¶ ê³ ë„í™”ë¥¼ ìœ„í•œ ê°œì„  ê³„íšì„ ì œì‹œí•©ë‹ˆë‹¤.

---

## 1. í˜„ì¬ ì•„í‚¤í…ì²˜ ë¶„ì„

### ë‹¨ì¼ ì—ì´ì „íŠ¸ ë£¨í”„

í˜„ì¬ HolmesGPTëŠ” í•˜ë‚˜ì˜ `ToolCallingLLM` ì¸ìŠ¤í„´ìŠ¤ê°€ ëª¨ë“  ë„êµ¬ë¥¼ ì§ì ‘ ê´€ë¦¬í•˜ëŠ” ë‹¨ì¼ ì—ì´ì „íŠ¸ êµ¬ì¡°ì…ë‹ˆë‹¤. LLMì´ ì‚¬ìš© ê°€ëŠ¥í•œ ì „ì²´ ë„êµ¬ ëª©ë¡ì„ ë§¤ í˜¸ì¶œë§ˆë‹¤ ì „ë‹¬ë°›ê³ , ì–´ë–¤ ë„êµ¬ë¥¼ í˜¸ì¶œí• ì§€ ìŠ¤ìŠ¤ë¡œ ê²°ì •í•©ë‹ˆë‹¤.

```mermaid
graph TD
    User[ì‚¬ìš©ì ì§ˆë¬¸ / ì•Œë¦¼] --> TCL[ToolCallingLLM]
    TCL --> LLM[LLM Provider\nLiteLLM]
    TCL --> TE[ToolExecutor]
    TE --> T1[kubernetes/core]
    TE --> T2[prometheus/metrics]
    TE --> T3[grafana/dashboards]
    TE --> T4[elasticsearch/logs]
    TE --> T5[argocd/core]
    TE --> TN[... 30+ ë„êµ¬ì…‹]

    LLM -.->|function calling| TCL
    TCL -.->|ë„êµ¬ í˜¸ì¶œ ê²°ê³¼| LLM

    style TCL fill:#e1f5fe
    style TE fill:#fff3e0
```

### í•µì‹¬ ì½”ë“œ íë¦„

ì¡°ì‚¬ ìš”ì²­ì´ ë“¤ì–´ì˜¤ë©´ ë‹¤ìŒê³¼ ê°™ì€ ìˆœì„œë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.

```mermaid
sequenceDiagram
    participant CLI as CLI / API
    participant Config as Config
    participant TCL as ToolCallingLLM
    participant LLM as LLM (LiteLLM)
    participant TE as ToolExecutor
    participant Tools as Toolsets

    CLI->>Config: create_toolcalling_llm()
    Config->>TE: ToolExecutor(toolsets)
    Config->>TCL: ToolCallingLLM(tool_executor, max_steps, llm)

    CLI->>TCL: call(messages)

    loop ìµœëŒ€ max_steps ë°˜ë³µ
        TCL->>TE: get_all_tools_openai_format()
        TE-->>TCL: ì „ì²´ ë„êµ¬ ëª©ë¡ (OpenAI format)
        TCL->>LLM: completion(messages, tools)
        LLM-->>TCL: ì‘ë‹µ (tool_calls ë˜ëŠ” í…ìŠ¤íŠ¸)

        alt ë„êµ¬ í˜¸ì¶œ ìš”ì²­
            TCL->>TE: get_tool_by_name(tool_name)
            TE-->>TCL: Tool ì¸ìŠ¤í„´ìŠ¤
            TCL->>Tools: tool.invoke(params, context)
            Tools-->>TCL: StructuredToolResult
            Note over TCL: ê²°ê³¼ë¥¼ messagesì— ì¶”ê°€
        else í…ìŠ¤íŠ¸ ì‘ë‹µ
            TCL-->>CLI: LLMResult (ìµœì¢… ê²°ë¡ )
        end
    end
```

### í•µì‹¬ íŒŒì¼ ë§µ

| íŒŒì¼ | ì—­í•  | ì£¼ìš” í´ë˜ìŠ¤/í•¨ìˆ˜ |
|------|------|------------------|
| `holmes/core/tool_calling_llm.py` | ì—ì´ì „íŠ¸ ë£¨í”„ ì—”ì§„ | `ToolCallingLLM.call()`, `IssueInvestigator.investigate()` |
| `holmes/core/tools.py` | ë„êµ¬/ë„êµ¬ì…‹ ì •ì˜ | `Tool`, `Toolset`, `YAMLTool`, `StructuredToolResult` |
| `holmes/core/tools_utils/tool_executor.py` | ë„êµ¬ ì‹¤í–‰ ê´€ë¦¬ | `ToolExecutor.get_all_tools_openai_format()` |
| `holmes/core/toolset_manager.py` | ë„êµ¬ì…‹ ë¡œë“œ/ê´€ë¦¬ | `ToolsetManager._list_all_toolsets()` |
| `holmes/config.py` | ì„¤ì • ë° íŒ©í† ë¦¬ | `Config.create_toolcalling_llm()` |
| `holmes/plugins/runbooks/__init__.py` | ëŸ°ë¶ ì¹´íƒˆë¡œê·¸ | `RunbookCatalog`, `load_runbook_catalog()` |

### í˜„ì¬ êµ¬ì¡°ì˜ í•œê³„

**ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ë¹„íš¨ìœ¨**

`ToolExecutor.get_all_tools_openai_format()`ì€ í™œì„±í™”ëœ ëª¨ë“  ë„êµ¬ë¥¼ OpenAI ë„êµ¬ ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜í•˜ì—¬ ë§¤ LLM í˜¸ì¶œì— í¬í•¨í•©ë‹ˆë‹¤. 30ê°œ ì´ìƒì˜ ë„êµ¬ ì •ì˜ê°€ ë§¤ë²ˆ ì „ë‹¬ë˜ë¯€ë¡œ, ì‹¤ì œ ì¡°ì‚¬ì— í•„ìš”í•˜ì§€ ì•Šì€ ë„êµ¬ë“¤ì˜ ìŠ¤í‚¤ë§ˆê°€ í”„ë¡¬í”„íŠ¸ í† í°ì„ ì†Œë¹„í•©ë‹ˆë‹¤.

**ë„ë©”ì¸ ì „ë¬¸í™” ë¶€ì¬**

ë‹¨ì¼ LLMì´ Kubernetes, Prometheus, Grafana, Elasticsearch ë“± ëª¨ë“  ë„ë©”ì¸ì˜ ë„êµ¬ë¥¼ ë™ì‹œì— ë‹¤ë£¹ë‹ˆë‹¤. ê° ë„ë©”ì¸ì— íŠ¹í™”ëœ í”„ë¡¬í”„íŠ¸ë‚˜ ì¡°ì‚¬ ì „ëµì„ ì ìš©í•  ìˆ˜ ì—†ìœ¼ë©°, ë„êµ¬ ìˆ˜ê°€ ì¦ê°€í• ìˆ˜ë¡ LLMì˜ ë„êµ¬ ì„ íƒ ì •í™•ë„ê°€ ì €í•˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ì™¸ë¶€ ë„êµ¬ ìƒíƒœê³„ í†µí•© ì–´ë ¤ì›€**

í˜„ì¬ ë„êµ¬ ì‹œìŠ¤í…œ(`Tool`, `Toolset`)ì€ HolmesGPT ê³ ìœ ì˜ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤. LangChain, CrewAI ë“± ì™¸ë¶€ ì—ì´ì „íŠ¸ í”„ë ˆì„ì›Œí¬ì˜ ë„êµ¬ë¥¼ ì§ì ‘ í™œìš©í•  ìˆ˜ ì—†ìœ¼ë©°, ë°˜ëŒ€ë¡œ Holmes ë„êµ¬ë¥¼ ì™¸ë¶€ í”„ë ˆì„ì›Œí¬ì—ì„œ ì¬ì‚¬ìš©í•˜ê¸°ë„ ì–´ë µìŠµë‹ˆë‹¤.

**ëŸ°ë¶ ì‹¤í–‰ì˜ í•œê³„**

í˜„ì¬ ëŸ°ë¶ì€ ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ë¥¼ LLMì˜ ì»¨í…ìŠ¤íŠ¸ì— ì£¼ì…í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. LLMì´ ëŸ°ë¶ì„ ìì—°ì–´ë¡œ í•´ì„í•˜ë¯€ë¡œ ë‹¨ê³„ ê±´ë„ˆë›°ê¸°, ì¡°ê±´ ë¶„ê¸° ë¯¸ì´í–‰ ë“±ì˜ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë©°, ì‹¤í–‰ ì¶”ì ì´ë‚˜ ì„±ê³µë¥  ë¶„ì„ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.

---

## 2. ë©€í‹° ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜

### Supervisor-Specialist íŒ¨í„´

ë‹¨ì¼ ì—ì´ì „íŠ¸ë¥¼ Supervisor(ì¡°ìœ¨ì)ì™€ ì—¬ëŸ¬ Specialist(ì „ë¬¸ê°€)ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤. Supervisorê°€ ì¡°ì‚¬ ì „ëµì„ ìˆ˜ë¦½í•˜ê³ , ê° ë„ë©”ì¸ì˜ Specialistì—ê²Œ ì„¸ë¶€ ì¡°ì‚¬ë¥¼ ìœ„ì„í•©ë‹ˆë‹¤.

```mermaid
graph TD
    User[ì‚¬ìš©ì ì§ˆë¬¸ / ì•Œë¦¼] --> Sup[SupervisorAgent]

    Sup -->|K8s ê´€ë ¨ ì¡°ì‚¬ ìœ„ì„| K8s[K8sSpecialist]
    Sup -->|ë©”íŠ¸ë¦­ ë¶„ì„ ìœ„ì„| Met[MetricsSpecialist]
    Sup -->|ë¡œê·¸ ë¶„ì„ ìœ„ì„| Log[LogsSpecialist]
    Sup -->|ë„¤íŠ¸ì›Œí¬ ì§„ë‹¨ ìœ„ì„| Net[NetworkSpecialist]
    Sup -->|í´ë¼ìš°ë“œ ì¸í”„ë¼ ìœ„ì„| Cloud[CloudSpecialist]
    Sup -->|DB ì§„ë‹¨ ìœ„ì„| DB[DatabaseSpecialist]

    K8s --> K8sTools[kubernetes/core\nkubernetes/logs]
    Met --> MetTools[prometheus/metrics\ngrafana/dashboards]
    Log --> LogTools[elasticsearch/logs\nloki/logs]
    Net --> NetTools[network/diagnostics\ncoredns/core]
    Cloud --> CloudTools[aws/core\ngcp/core]
    DB --> DBTools[postgresql/core\nredis/core]

    Sup --> State[InvestigationState\nê³µìœ  ìƒíƒœ]
    K8s -.-> State
    Met -.-> State
    Log -.-> State
    Net -.-> State
    Cloud -.-> State
    DB -.-> State

    style Sup fill:#e8f5e9
    style State fill:#fce4ec
```

### ì—ì´ì „íŠ¸ í´ë˜ìŠ¤ ì„¤ê³„

```mermaid
classDiagram
    class BaseAgent {
        +name: str
        +llm: LLM
        +tool_executor: ToolExecutor
        +call(messages) LLMResult
    }

    class SupervisorAgent {
        +registry: AgentRegistry
        +router: AgentRouter
        +state: InvestigationState
        +investigate(question) LLMResult
        -_plan_investigation() list[AgentTask]
        -_delegate(task: AgentTask) LLMResult
        -_synthesize(results) str
    }

    class SpecialistAgent {
        +domain: str
        +domain_prompt: str
        +investigate_subtask(task: AgentTask) LLMResult
    }

    class AgentRegistry {
        +agents: dict[str, SpecialistAgent]
        +register(agent: SpecialistAgent)
        +get(domain: str) SpecialistAgent
        +list_domains() list[str]
    }

    class AgentRouter {
        +registry: AgentRegistry
        +route(task: AgentTask) SpecialistAgent
        -_classify_domain(task) str
    }

    class InvestigationState {
        +question: str
        +findings: list[Finding]
        +tool_calls: list[ToolCallResult]
        +add_finding(finding: Finding)
        +get_summary() str
    }

    class AgentTask {
        +description: str
        +domain: str
        +context: dict
        +parent_task_id: str
    }

    BaseAgent <|-- SupervisorAgent
    BaseAgent <|-- SpecialistAgent
    SupervisorAgent --> AgentRegistry
    SupervisorAgent --> AgentRouter
    SupervisorAgent --> InvestigationState
    AgentRouter --> AgentRegistry
    SpecialistAgent ..> InvestigationState
    SupervisorAgent ..> AgentTask
```

### ìœ„ì„ ë„êµ¬ ì„¤ê³„

SupervisorëŠ” `delegate_to_specialist` ë„êµ¬ë¥¼ í†µí•´ Specialistì—ê²Œ ì‘ì—…ì„ ìœ„ì„í•©ë‹ˆë‹¤. ì´ ë„êµ¬ëŠ” Supervisorì˜ LLMì´ function callingìœ¼ë¡œ í˜¸ì¶œí•˜ëŠ” ì¼ë°˜ ë„êµ¬ì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ë¥¼ ê°–ìŠµë‹ˆë‹¤.

```python
# ì˜ì‚¬ ì½”ë“œ - SupervisorAgentì˜ ìœ„ì„ ë„êµ¬
class DelegateToSpecialist(Tool):
    name = "delegate_to_specialist"
    description = "ë„ë©”ì¸ ì „ë¬¸ ì—ì´ì „íŠ¸ì—ê²Œ ì„¸ë¶€ ì¡°ì‚¬ë¥¼ ìœ„ì„í•©ë‹ˆë‹¤."
    parameters = {
        "domain": ToolParameter(
            description="ì¡°ì‚¬ë¥¼ ìœ„ì„í•  ë„ë©”ì¸ (kubernetes, metrics, logs, network, cloud, database)",
            type="string",
            enum=["kubernetes", "metrics", "logs", "network", "cloud", "database"],
        ),
        "task_description": ToolParameter(
            description="ì „ë¬¸ ì—ì´ì „íŠ¸ê°€ ìˆ˜í–‰í•  êµ¬ì²´ì ì¸ ì¡°ì‚¬ ë‚´ìš©",
            type="string",
        ),
    }

    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
        domain = params["domain"]
        task = AgentTask(
            description=params["task_description"],
            domain=domain,
            context=self.state.get_summary(),
        )
        specialist = self.registry.get(domain)
        result = specialist.investigate_subtask(task)
        self.state.add_finding(Finding(domain=domain, content=result.result))
        return StructuredToolResult(
            status=StructuredToolResultStatus.SUCCESS,
            data=result.result,
        )
```

### ë„ë©”ì¸-ë„êµ¬ì…‹ ë§¤í•‘

ê° Specialistê°€ ê´€ë¦¬í•˜ëŠ” ë„êµ¬ì…‹ê³¼ ì—­í• ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

| ë„ë©”ì¸ | Specialist | ë‹´ë‹¹ ë„êµ¬ì…‹ | ì—­í•  |
|--------|-----------|-------------|------|
| kubernetes | K8sSpecialist | `kubernetes/core`, `kubernetes/logs` | íŒŒë“œ/ë””í”Œë¡œì´ë¨¼íŠ¸ ìƒíƒœ, ì´ë²¤íŠ¸, ë¡œê·¸ |
| metrics | MetricsSpecialist | `prometheus/metrics`, `grafana/dashboards` | ë©”íŠ¸ë¦­ ì¿¼ë¦¬, ëŒ€ì‹œë³´ë“œ ë¶„ì„, ì´ìƒ íƒì§€ |
| logs | LogsSpecialist | `elasticsearch/logs`, `loki/logs` | ë¡œê·¸ ê²€ìƒ‰, íŒ¨í„´ ë¶„ì„, ì—ëŸ¬ ì¶”ì¶œ |
| network | NetworkSpecialist | `network/diagnostics`, `coredns/core` | DNS, ì—°ê²°ì„±, ë„¤íŠ¸ì›Œí¬ ì •ì±… ì§„ë‹¨ |
| cloud | CloudSpecialist | `aws/core`, `gcp/core` | í´ë¼ìš°ë“œ ë¦¬ì†ŒìŠ¤ ìƒíƒœ, IAM, ë„¤íŠ¸ì›Œí¬ ì„¤ì • |
| database | DatabaseSpecialist | `postgresql/core`, `redis/core` | DB ì—°ê²°, ì¿¼ë¦¬ ì„±ëŠ¥, ë³µì œ ìƒíƒœ |

### ê¸°ì¡´ ë‹¨ì¼ ì—ì´ì „íŠ¸ì™€ì˜ í˜¸í™˜ì„±

`agent_mode` ì„¤ì •ì„ í†µí•´ ë‹¨ì¼/ë©€í‹° ì—ì´ì „íŠ¸ ëª¨ë“œë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ `single`ë¡œ ê¸°ì¡´ ë™ì‘ì„ ìœ ì§€í•©ë‹ˆë‹¤.

```yaml
# config.yaml
agent_mode: single  # single | multi

# ë©€í‹° ì—ì´ì „íŠ¸ ëª¨ë“œ ì„¤ì •
multi_agent:
  enabled_specialists:
    - kubernetes
    - metrics
    - logs
  max_delegation_depth: 2
  parallel_delegation: true
```

```python
# ì˜ì‚¬ ì½”ë“œ - Configì— agent_mode ì¶”ê°€
class Config(RobustaBaseConfig):
    agent_mode: str = "single"  # "single" | "multi"

    def create_toolcalling_llm(self, dal=None, model=None, tracer=None):
        tool_executor = self.create_tool_executor(dal)
        llm = self._get_llm(model, tracer)

        if self.agent_mode == "multi":
            return self._create_supervisor_agent(tool_executor, llm)
        else:
            return ToolCallingLLM(tool_executor, self.max_steps, llm)

    def _create_supervisor_agent(self, tool_executor, llm):
        registry = AgentRegistry()
        # ë„ë©”ì¸ë³„ Specialist ìƒì„± - ê° SpecialistëŠ” í•´ë‹¹ ë„ë©”ì¸ì˜ ë„êµ¬ì…‹ë§Œ í¬í•¨
        for domain, toolset_names in DOMAIN_TOOLSET_MAP.items():
            domain_toolsets = [
                ts for ts in tool_executor.toolsets
                if ts.name in toolset_names and ts.status == ToolsetStatusEnum.ENABLED
            ]
            if domain_toolsets:
                specialist = SpecialistAgent(
                    domain=domain,
                    llm=llm,
                    tool_executor=ToolExecutor(domain_toolsets),
                )
                registry.register(specialist)

        return SupervisorAgent(
            registry=registry,
            llm=llm,
            tool_executor=tool_executor,
            max_steps=self.max_steps,
        )
```

### ì¡°ì‚¬ íë¦„ ì˜ˆì‹œ

"checkout-api íŒŒë“œê°€ CrashLoopBackOff ìƒíƒœì…ë‹ˆë‹¤"ë¼ëŠ” ì•Œë¦¼ì— ëŒ€í•œ ë©€í‹° ì—ì´ì „íŠ¸ ì¡°ì‚¬ íë¦„ì…ë‹ˆë‹¤.

```mermaid
sequenceDiagram
    participant User as ì‚¬ìš©ì
    participant Sup as SupervisorAgent
    participant K8s as K8sSpecialist
    participant Met as MetricsSpecialist
    participant State as InvestigationState

    User->>Sup: CrashLoopBackOff ì›ì¸ ë¶„ì„ ìš”ì²­

    Note over Sup: ì¡°ì‚¬ ì „ëµ ìˆ˜ë¦½<br/>1. K8s íŒŒë“œ ìƒíƒœ í™•ì¸<br/>2. ë©”íŠ¸ë¦­ ë¶„ì„

    Sup->>K8s: delegate("íŒŒë“œ ìƒíƒœ ë° ë¡œê·¸ í™•ì¸")
    K8s->>K8s: kubectl_get_pods
    K8s->>K8s: kubectl_logs
    K8s->>State: Finding: OOMKilled í™•ì¸
    K8s-->>Sup: "íŒŒë“œê°€ OOMKilledë¡œ ì¬ì‹œì‘ ë°˜ë³µ"

    Sup->>Met: delegate("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì´ í™•ì¸")
    Met->>Met: prometheus_query (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰)
    Met->>State: Finding: ë©”ëª¨ë¦¬ limit ë„ë‹¬ íŒ¨í„´
    Met-->>Sup: "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ limitì— ì§€ì† ë„ë‹¬"

    Note over Sup: ê²°ê³¼ ì¢…í•© (Synthesis)
    Sup->>State: get_summary()
    Sup-->>User: ê·¼ë³¸ ì›ì¸: ë©”ëª¨ë¦¬ limit ë¶€ì¡±<br/>ê¶Œì¥: memory limit ìƒí–¥
```

---

## 3. LangChain ë„êµ¬ í†µí•©

### í†µí•© ëª©í‘œ

HolmesGPT ë„êµ¬ì™€ LangChain ë„êµ¬ ê°„ì˜ ì–‘ë°©í–¥ ì–´ëŒ‘í„°ë¥¼ êµ¬ì¶•í•˜ì—¬, ë‘ ìƒíƒœê³„ì˜ ë„êµ¬ë¥¼ ìƒí˜¸ í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

```mermaid
graph LR
    subgraph HolmesGPT
        HT[Holmes Tool]
        HTS[Holmes Toolset]
    end

    subgraph Adapter Layer
        H2L[HolmesToLangChainAdapter]
        L2H[LangChainToHolmesAdapter]
    end

    subgraph LangChain
        LBT[LangChain BaseTool]
        LCT[Community Tools]
    end

    HT --> H2L --> LBT
    LBT --> L2H --> HT
    LCT --> L2H

    style H2L fill:#e8f5e9
    style L2H fill:#e8f5e9
```

### Holmes Tool â†’ LangChain BaseTool ì–´ëŒ‘í„°

Holmes ë„êµ¬ë¥¼ LangChainì˜ `BaseTool` ì¸í„°í˜ì´ìŠ¤ë¡œ ë˜í•‘í•˜ì—¬, LangGraph ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì´ë‚˜ LangChain ì—ì´ì „íŠ¸ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

```python
# ì˜ì‚¬ ì½”ë“œ - HolmesToLangChainAdapter
from typing import Optional, Type
from pydantic import BaseModel, Field

# LangChainì€ ì„ íƒì  ì˜ì¡´ì„± (lazy import)
def _import_langchain():
    try:
        from langchain_core.tools import BaseTool as LCBaseTool
        return LCBaseTool
    except ImportError:
        raise ImportError(
            "langchain-coreê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install holmesgpt[langchain]"
        )

class HolmesToLangChainAdapter:
    """Holmes Toolì„ LangChain BaseToolë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""

    @staticmethod
    def adapt(holmes_tool: Tool, llm: LLM) -> "LCBaseTool":
        LCBaseTool = _import_langchain()

        # Holmes Toolì˜ íŒŒë¼ë¯¸í„°ë¥¼ Pydantic ëª¨ë¸ë¡œ ë³€í™˜
        field_definitions = {}
        for param_name, param in holmes_tool.parameters.items():
            field_definitions[param_name] = (
                str,
                Field(description=param.description),
            )
        ArgsSchema = type(
            f"{holmes_tool.name}_args",
            (BaseModel,),
            {"__annotations__": {k: str for k in field_definitions}},
        )

        class WrappedTool(LCBaseTool):
            name: str = holmes_tool.name
            description: str = holmes_tool.description
            args_schema: Type[BaseModel] = ArgsSchema

            def _run(self, **kwargs) -> str:
                context = ToolInvokeContext(
                    llm=llm,
                    max_token_count=llm.get_max_token_count_for_single_tool(),
                    tool_name=holmes_tool.name,
                    tool_call_id="langchain_call",
                )
                result = holmes_tool.invoke(kwargs, context)
                return result.get_stringified_data()

        return WrappedTool()
```

### LangChain BaseTool â†’ Holmes Tool ì–´ëŒ‘í„°

LangChain ì»¤ë®¤ë‹ˆí‹° ë„êµ¬ë¥¼ Holmes ë„êµ¬ ì¸í„°í˜ì´ìŠ¤ë¡œ ë˜í•‘í•˜ì—¬, ê¸°ì¡´ `ToolExecutor`ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

```python
# ì˜ì‚¬ ì½”ë“œ - LangChainToHolmesAdapter
class LangChainToHolmesAdapter(Tool):
    """LangChain BaseToolì„ Holmes Toolë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    name: str
    description: str

    def __init__(self, lc_tool: "LCBaseTool"):
        # LangChain ë„êµ¬ì˜ ë©”íƒ€ë°ì´í„°ë¡œ Holmes Tool ì´ˆê¸°í™”
        parameters = {}
        if hasattr(lc_tool, "args_schema") and lc_tool.args_schema:
            for field_name, field_info in lc_tool.args_schema.model_fields.items():
                parameters[field_name] = ToolParameter(
                    description=field_info.description or "",
                    type="string",
                    required=field_info.is_required(),
                )
        super().__init__(
            name=f"lc_{lc_tool.name}",
            description=lc_tool.description,
            parameters=parameters,
        )
        self._lc_tool = lc_tool

    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
        try:
            result = self._lc_tool.run(params)
            return StructuredToolResult(
                status=StructuredToolResultStatus.SUCCESS,
                data=str(result),
                params=params,
            )
        except Exception as e:
            return StructuredToolResult(
                status=StructuredToolResultStatus.ERROR,
                error=str(e),
                params=params,
            )

    def get_parameterized_one_liner(self, params: dict) -> str:
        return f"lc_{self._lc_tool.name}({params})"
```

### LangGraph ê¸°ë°˜ ë©€í‹° ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜

ë©€í‹° ì—ì´ì „íŠ¸ ëª¨ë“œì—ì„œ LangGraphì˜ `StateGraph`ë¥¼ í™œìš©í•˜ë©´, ì—ì´ì „íŠ¸ ê°„ ìƒíƒœ ì „ì´ë¥¼ ì„ ì–¸ì ìœ¼ë¡œ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# ì˜ì‚¬ ì½”ë“œ - LangGraph StateGraph ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
from typing import TypedDict, Annotated
import operator

# LangGraphëŠ” ì„ íƒì  ì˜ì¡´ì„±
def _import_langgraph():
    try:
        from langgraph.graph import StateGraph, END
        return StateGraph, END
    except ImportError:
        raise ImportError(
            "langgraphê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install holmesgpt[langgraph]"
        )

class InvestigationGraphState(TypedDict):
    question: str
    plan: list[str]
    findings: Annotated[list[dict], operator.add]
    final_answer: str

def build_investigation_graph(supervisor, specialists):
    StateGraph, END = _import_langgraph()

    graph = StateGraph(InvestigationGraphState)

    # ë…¸ë“œ ì •ì˜
    graph.add_node("plan", supervisor.plan_node)
    for domain, specialist in specialists.items():
        graph.add_node(f"investigate_{domain}", specialist.investigate_node)
    graph.add_node("synthesize", supervisor.synthesize_node)

    # ì—£ì§€ ì •ì˜ (plan â†’ ë³‘ë ¬ ì¡°ì‚¬ â†’ ì¢…í•©)
    graph.add_edge("plan", "investigate_kubernetes")
    graph.add_edge("plan", "investigate_metrics")
    graph.add_edge("investigate_kubernetes", "synthesize")
    graph.add_edge("investigate_metrics", "synthesize")
    graph.add_edge("synthesize", END)

    graph.set_entry_point("plan")
    return graph.compile()
```

### config.yaml ì„¤ì •

```yaml
# config.yaml - LangChain ì»¤ë®¤ë‹ˆí‹° ë„êµ¬ í™œì„±í™”
langchain_tools:
  enabled: true
  tools:
    - name: "wikipedia"
      package: "langchain_community.tools.wikipedia"
      class: "WikipediaQueryRun"
    - name: "arxiv"
      package: "langchain_community.tools.arxiv"
      class: "ArxivQueryRun"
      config:
        top_k_results: 3
```

### ì„ íƒì  ì˜ì¡´ì„± ì „ëµ

LangChain í†µí•©ì€ ì„ íƒì  ì˜ì¡´ì„±ìœ¼ë¡œ ê´€ë¦¬í•˜ì—¬, LangChainì´ ì„¤ì¹˜ë˜ì§€ ì•Šì€ í™˜ê²½ì—ì„œë„ ê¸°ì¡´ ê¸°ëŠ¥ì´ ì •ìƒ ì‘ë™í•˜ë„ë¡ í•©ë‹ˆë‹¤.

```toml
# pyproject.toml
[tool.poetry.extras]
langchain = ["langchain-core", "langchain-community"]
langgraph = ["langgraph", "langchain-core"]
all = ["langchain-core", "langchain-community", "langgraph"]
```

```python
# ì˜ì‚¬ ì½”ë“œ - ì§€ì—° ë¡œë”© íŒ¨í„´
LANGCHAIN_AVAILABLE = False
try:
    import langchain_core
    LANGCHAIN_AVAILABLE = True
except ImportError:
    pass

def create_langchain_tools(config):
    if not LANGCHAIN_AVAILABLE:
        logging.warning(
            "langchain-coreê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
            "LangChain ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´: pip install holmesgpt[langchain]"
        )
        return []
    # LangChain ë„êµ¬ ìƒì„± ë¡œì§
    ...
```

---

## 4. ëŸ°ë¶ ì‹œìŠ¤í…œ ê°œì„ 

### í˜„ì¬ ëŸ°ë¶ì˜ í•œê³„

í˜„ì¬ ëŸ°ë¶ì€ ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ë¥¼ LLM ì»¨í…ìŠ¤íŠ¸ì— ì£¼ì…í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ì´ ë°©ì‹ì˜ í•œê³„ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

- **ì‹¤í–‰ ì¶”ì  ë¶ˆê°€**: ëŸ°ë¶ì˜ ì–´ë–¤ ë‹¨ê³„ê°€ ì‹¤í–‰ë˜ì—ˆëŠ”ì§€, ê° ë‹¨ê³„ì˜ ì„±ê³µ/ì‹¤íŒ¨ ì—¬ë¶€ë¥¼ í”„ë¡œê·¸ë˜ë°ì ìœ¼ë¡œ ì¶”ì í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
- **ì¡°ê±´ ë¶„ê¸° ë¶ˆí™•ì‹¤**: LLMì´ ìì—°ì–´ë¥¼ í•´ì„í•˜ë¯€ë¡œ ì¡°ê±´ë¶€ ë¶„ê¸°ë¥¼ ì •í™•íˆ ì´í–‰í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **ì„±ê³¼ ë¶„ì„ ë¶ˆê°€**: ëŸ°ë¶ë³„ ì„±ê³µë¥ , í‰ê·  ì¡°ì‚¬ ì‹œê°„, ìì£¼ ì‹¤íŒ¨í•˜ëŠ” ë‹¨ê³„ ë“±ì„ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
- **ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° í‘œí˜„ í•œê³„**: ë³‘ë ¬ ì‹¤í–‰, ì—ì´ì „íŠ¸ ìœ„ì„, íƒ€ì„ì•„ì›ƒ ë“± ê³ ê¸‰ ì›Œí¬í”Œë¡œìš°ë¥¼ í‘œí˜„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

### êµ¬ì¡°í™”ëœ YAML ëŸ°ë¶ í˜•ì‹

ê¸°ì¡´ ë§ˆí¬ë‹¤ìš´ ëŸ°ë¶ê³¼ í˜¸í™˜ì„ ìœ ì§€í•˜ë©´ì„œ, ìƒˆë¡œìš´ YAML ê¸°ë°˜ êµ¬ì¡°í™”ëœ ëŸ°ë¶ í˜•ì‹(`runbook/v1`)ì„ ë„ì…í•©ë‹ˆë‹¤.

```yaml
# êµ¬ì¡°í™”ëœ ëŸ°ë¶ ì˜ˆì‹œ - spark-job-failure.yaml
apiVersion: runbook/v1
metadata:
  id: spark-job-failure
  title: Spark ì‘ì—… ì‹¤íŒ¨ ì§„ë‹¨
  description: SparkApplication FAILED ìƒíƒœ ì›ì¸ ë¶„ì„
  update_date: "2025-06-15"
  tags: ["spark", "kubernetes", "data-engineering"]

goal: |
  SparkApplicationì´ FAILED ìƒíƒœì¼ ë•Œ ê·¼ë³¸ ì›ì¸ì„ íŒŒì•…í•©ë‹ˆë‹¤.
  Spark Operator í™˜ê²½ì—ì„œ ë“œë¼ì´ë²„ ë˜ëŠ” Executor ìˆ˜ì¤€ì˜ ì‹¤íŒ¨ ì›ì¸ì„ ì²´ê³„ì ìœ¼ë¡œ ì¡°ì‚¬í•©ë‹ˆë‹¤.

workflow:
  - id: check_app_status
    description: "SparkApplication ë¦¬ì†ŒìŠ¤ì˜ ìƒì„¸ ìƒíƒœ í™•ì¸"
    domain: kubernetes
    expected_fields: ["status.applicationState", "status.terminationTime"]

  - id: check_driver_pod
    description: "ë“œë¼ì´ë²„ íŒŒë“œ ìƒíƒœ í™•ì¸ (OOMKilled, Error ë“±)"
    domain: kubernetes
    depends_on: [check_app_status]

  - id: check_driver_logs
    description: "ë“œë¼ì´ë²„ íŒŒë“œ ë¡œê·¸ì—ì„œ ì—ëŸ¬ ë©”ì‹œì§€ í™•ì¸"
    domain: kubernetes
    depends_on: [check_driver_pod]
    error_patterns:
      - "OutOfMemoryError"
      - "ClassNotFoundException"
      - "SparkException"

  - id: check_executor_pods
    description: "Executor íŒŒë“œ ìƒíƒœ ë° ë¡œê·¸ í™•ì¸"
    domain: kubernetes
    depends_on: [check_app_status]
    condition: "driver_pod_status != 'OOMKilled'"

  - id: check_k8s_events
    description: "ê´€ë ¨ Kubernetes ì´ë²¤íŠ¸ í™•ì¸"
    domain: kubernetes
    depends_on: [check_app_status]

  - id: check_resources
    description: "ë“œë¼ì´ë²„/Executor ë¦¬ì†ŒìŠ¤ requests/limits í™•ì¸"
    domain: kubernetes
    depends_on: [check_driver_pod, check_executor_pods]

synthesis:
  correlation_rules:
    - condition: "driver_logs contains 'OutOfMemoryError'"
      conclusion: "ë“œë¼ì´ë²„ Java heap ë©”ëª¨ë¦¬ ë¶€ì¡±"
      confidence: high
    - condition: "executor_pod_status == 'OOMKilled'"
      conclusion: "Executor ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ì‘ì—… ì‹¤íŒ¨"
      confidence: high
    - condition: "k8s_events contains 'Insufficient'"
      conclusion: "í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤ ë¶€ì¡±ìœ¼ë¡œ ìŠ¤ì¼€ì¤„ë§ ì‹¤íŒ¨"
      confidence: medium

remediation:
  actions:
    - condition: "root_cause == 'OOMKilled'"
      immediate: "ë©”ëª¨ë¦¬ limits ì¦ê°€ ë˜ëŠ” memoryOverhead ì¡°ì •"
      permanent: "ë°ì´í„° íŒŒí‹°ì…”ë‹ ìµœì í™”, Executor ìˆ˜ ì¡°ì •"
    - condition: "root_cause == 'ClassNotFoundException'"
      immediate: "JAR ì˜ì¡´ì„± í™•ì¸ ë° spark.jars ì„¤ì • ì ê²€"
  escalation:
    threshold: "ë™ì¼ ì‘ì—… 3íšŒ ì—°ì† ì‹¤íŒ¨"
    target: "ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ íŒ€"
```

### RunbookExecutor

êµ¬ì¡°í™”ëœ ëŸ°ë¶ì„ ë‹¨ê³„ë³„ë¡œ ì‹¤í–‰í•˜ëŠ” ì—”ì§„ì…ë‹ˆë‹¤. ê¸°ì¡´ ë§ˆí¬ë‹¤ìš´ ëŸ°ë¶ì€ í˜„ì¬ ë°©ì‹(LLM ì»¨í…ìŠ¤íŠ¸ ì£¼ì…)ìœ¼ë¡œ ê³„ì† ì²˜ë¦¬í•˜ê³ , `runbook/v1` í˜•ì‹ì˜ ëŸ°ë¶ë§Œ `RunbookExecutor`ê°€ ì²˜ë¦¬í•©ë‹ˆë‹¤.

```python
# ì˜ì‚¬ ì½”ë“œ - RunbookExecutor
class RunbookStep(BaseModel):
    id: str
    description: str
    domain: str
    depends_on: list[str] = []
    condition: Optional[str] = None
    status: str = "pending"  # pending, running, completed, skipped, failed
    result: Optional[str] = None

class RunbookExecutor:
    def __init__(self, runbook: StructuredRunbook, agent_registry: AgentRegistry):
        self.runbook = runbook
        self.registry = agent_registry
        self.tracker = RunbookTracker(runbook.metadata.id)
        self.steps = {s.id: RunbookStep(**s.model_dump()) for s in runbook.workflow}

    def execute(self, state: InvestigationState) -> RunbookResult:
        self.tracker.start()
        for step in self._topological_sort():
            # ì˜ì¡´ì„± í™•ì¸
            if not self._dependencies_met(step):
                step.status = "skipped"
                continue

            # ì¡°ê±´ í‰ê°€
            if step.condition and not self._evaluate_condition(step.condition, state):
                step.status = "skipped"
                self.tracker.record_step(step)
                continue

            # ì—ì´ì „íŠ¸ ìœ„ì„ ì‹¤í–‰
            step.status = "running"
            specialist = self.registry.get(step.domain)
            if specialist:
                result = specialist.investigate_subtask(
                    AgentTask(description=step.description, domain=step.domain)
                )
                step.result = result.result
                step.status = "completed"
            else:
                step.status = "failed"
                step.result = f"ë„ë©”ì¸ '{step.domain}'ì— í•´ë‹¹í•˜ëŠ” Specialistê°€ ì—†ìŠµë‹ˆë‹¤."

            state.add_finding(Finding(step_id=step.id, content=step.result))
            self.tracker.record_step(step)

        # Synthesis
        conclusion = self._synthesize(state)
        self.tracker.complete(conclusion)
        return RunbookResult(steps=self.steps, conclusion=conclusion)

    def _topological_sort(self) -> list[RunbookStep]:
        """depends_on ê´€ê³„ì— ë”°ë¼ ì‹¤í–‰ ìˆœì„œë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
        ...

    def _evaluate_condition(self, condition: str, state: InvestigationState) -> bool:
        """ì¡°ê±´ë¬¸ì„ ìƒíƒœ ì»¨í…ìŠ¤íŠ¸ì—ì„œ í‰ê°€í•©ë‹ˆë‹¤."""
        ...
```

### ë™ì  ëŸ°ë¶ ì„ íƒ

í˜„ì¬ëŠ” LLMì´ ì¹´íƒˆë¡œê·¸ì˜ descriptionì„ ë³´ê³  ëŸ°ë¶ì„ ì„ íƒí•©ë‹ˆë‹¤. ì´ë¥¼ í™•ì¥í•˜ì—¬ í‚¤ì›Œë“œ ë§¤ì¹­, ì•Œë¦¼ ë ˆì´ë¸” ë§¤ì¹­, LLM íŒë‹¨ì„ ê²°í•©í•œ ë‹¤ë‹¨ê³„ ì„ íƒ ë©”ì»¤ë‹ˆì¦˜ì„ ë„ì…í•©ë‹ˆë‹¤.

```mermaid
flowchart TD
    A[ì•Œë¦¼ / ì§ˆë¬¸ ìˆ˜ì‹ ] --> B{í‚¤ì›Œë“œ ë§¤ì¹­}
    B -- ì¼ì¹˜ --> C[í›„ë³´ ëŸ°ë¶ ëª©ë¡]
    B -- ë¶ˆì¼ì¹˜ --> D{ì•Œë¦¼ ë ˆì´ë¸” ë§¤ì¹­}
    D -- ì¼ì¹˜ --> C
    D -- ë¶ˆì¼ì¹˜ --> E{LLM íŒë‹¨}
    E --> C

    C --> F{í›„ë³´ ìˆ˜}
    F -- 1ê°œ --> G[ëŸ°ë¶ ë¡œë“œ]
    F -- 2ê°œ ì´ìƒ --> H[LLMì´ ìµœì  ëŸ°ë¶ ì„ íƒ]
    F -- 0ê°œ --> I[ëŸ°ë¶ ì—†ì´ ììœ  ì¡°ì‚¬]

    H --> G
    G --> J[RunbookExecutor ë˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ì£¼ì…]

    style C fill:#e8f5e9
    style G fill:#e1f5fe
```

### ì‹¤í–‰ ì¶”ì  ë° ë¶„ì„

```python
# ì˜ì‚¬ ì½”ë“œ - RunbookTracker / RunbookAnalytics
class RunbookTracker:
    def __init__(self, runbook_id: str):
        self.runbook_id = runbook_id
        self.execution_id = str(uuid.uuid4())
        self.started_at = None
        self.completed_at = None
        self.step_records: list[StepRecord] = []

    def start(self):
        self.started_at = datetime.utcnow()

    def record_step(self, step: RunbookStep):
        self.step_records.append(StepRecord(
            step_id=step.id,
            status=step.status,
            timestamp=datetime.utcnow(),
            result_summary=step.result[:200] if step.result else None,
        ))

    def complete(self, conclusion: str):
        self.completed_at = datetime.utcnow()
        # ì‹¤í–‰ ê²°ê³¼ë¥¼ ì €ì¥ (íŒŒì¼ ë˜ëŠ” DB)

class RunbookAnalytics:
    def get_success_rate(self, runbook_id: str) -> float:
        """ëŸ°ë¶ë³„ ì„±ê³µë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
        ...

    def get_avg_duration(self, runbook_id: str) -> timedelta:
        """ëŸ°ë¶ë³„ í‰ê·  ì‹¤í–‰ ì‹œê°„ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
        ...

    def get_failure_hotspots(self, runbook_id: str) -> list[dict]:
        """ìì£¼ ì‹¤íŒ¨í•˜ëŠ” ë‹¨ê³„ë¥¼ ì‹ë³„í•©ë‹ˆë‹¤."""
        ...
```

### ìë™ ë³µêµ¬ (Remediation) ê¸°ëŠ¥

í˜„ì¬ HolmesGPTëŠ” ì½ê¸° ì „ìš© ë„êµ¬ë§Œ ì‚¬ìš©í•˜ë©°, `restricted` ì†ì„±ì´ `True`ì¸ ë„êµ¬ëŠ” ëŸ°ë¶ ì‚¬ìš© ì‹œì—ë§Œ í™œì„±í™”ë©ë‹ˆë‹¤. ì´ ê¸°ì¡´ ë©”ì»¤ë‹ˆì¦˜ì„ í™•ì¥í•˜ì—¬, êµ¬ì¡°í™”ëœ ëŸ°ë¶ì˜ remediation ì„¹ì…˜ì— ì •ì˜ëœ ì¡°ì¹˜ë¥¼ ìŠ¹ì¸ í›„ ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

```mermaid
flowchart TD
    A[ëŸ°ë¶ Remediation ì„¹ì…˜] --> B{ìë™ ë³µêµ¬ ê°€ëŠ¥?}
    B -- ì˜ˆ --> C[ë³µêµ¬ ë„êµ¬ í˜¸ì¶œ ì¤€ë¹„]
    C --> D{ì‚¬ìš©ì ìŠ¹ì¸ í•„ìš”?}
    D -- approval_required --> E[ì‚¬ìš©ìì—ê²Œ ìŠ¹ì¸ ìš”ì²­\nApprovalRequirement]
    D -- ì‚¬ì „ ìŠ¹ì¸ë¨ --> F[ë³µêµ¬ ë„êµ¬ ì‹¤í–‰]
    E -- ìŠ¹ì¸ --> F
    E -- ê±°ë¶€ --> G[ë³µêµ¬ ê±´ë„ˆëœ€\nê¶Œê³ ì‚¬í•­ë§Œ ì¶œë ¥]
    F --> H[ë³µêµ¬ ê²°ê³¼ í™•ì¸]
    B -- ì•„ë‹ˆì˜¤ --> G

    style E fill:#fff3e0
    style F fill:#e8f5e9
```

ê¸°ì¡´ `Tool.restricted` ì†ì„±ê³¼ `ToolCallingLLM._runbook_in_use` í”Œë˜ê·¸ë¥¼ í™œìš©í•˜ë¯€ë¡œ, ë³´ì•ˆ ëª¨ë¸ì„ ë³€ê²½í•˜ì§€ ì•Šê³  ë³µêµ¬ ê¸°ëŠ¥ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ëŸ°ë¶ ë²„ì „ ê´€ë¦¬ ë° A/B í…ŒìŠ¤íŠ¸

ë™ì¼ ì¥ì•  ìœ í˜•ì— ëŒ€í•´ ì—¬ëŸ¬ ë²„ì „ì˜ ëŸ°ë¶ì„ ê´€ë¦¬í•˜ê³ , ì„±ê³¼ë¥¼ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```yaml
# catalog.json í™•ì¥ - ë²„ì „ ê´€ë¦¬
{
  "catalog": [
    {
      "id": "spark-job-failure",
      "update_date": "2025-08-01",
      "description": "SparkApplication FAILED ìƒíƒœ ì›ì¸ ë¶„ì„",
      "link": "spark-job-failure.yaml",
      "format": "runbook/v1",
      "version": "2.0",
      "ab_test": {
        "enabled": true,
        "weight": 0.7
      }
    },
    {
      "id": "spark-job-failure-legacy",
      "update_date": "2025-06-15",
      "description": "SparkApplication FAILED ìƒíƒœ ì›ì¸ ë¶„ì„ (ê¸°ì¡´ ë²„ì „)",
      "link": "spark-job-failure.md",
      "format": "markdown",
      "version": "1.0",
      "ab_test": {
        "enabled": true,
        "weight": 0.3
      }
    }
  ]
}
```

---

## 5. êµ¬í˜„ ë¡œë“œë§µ

### Phase 1: ê¸°ë°˜ êµ¬ì¶• (6-8ì£¼)

**ëª©í‘œ**: ì–´ëŒ‘í„° ê³„ì¸µê³¼ ì—ì´ì „íŠ¸ ê¸°ë³¸ í´ë˜ìŠ¤ë¥¼ êµ¬í˜„í•˜ì—¬, ë©€í‹° ì—ì´ì „íŠ¸ ì „í™˜ì˜ ê¸°ë°˜ì„ ë§ˆë ¨í•©ë‹ˆë‹¤.

- `BaseAgent` ì¶”ìƒ í´ë˜ìŠ¤ êµ¬í˜„
- `InvestigationState` ê³µìœ  ìƒíƒœ ëª¨ë¸ êµ¬í˜„
- `HolmesToLangChainAdapter`, `LangChainToHolmesAdapter` êµ¬í˜„
- ì„ íƒì  ì˜ì¡´ì„± (`pyproject.toml` extras) ì„¤ì •
- `ToolsetManager`ì— `get_toolsets_by_domain()` ë©”ì„œë“œ ì¶”ê°€
- ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ë° ì–´ëŒ‘í„° í†µí•© í…ŒìŠ¤íŠ¸

### Phase 2: ë©€í‹° ì—ì´ì „íŠ¸ (8-10ì£¼)

**ëª©í‘œ**: Supervisor/Specialist íŒ¨í„´ì„ êµ¬í˜„í•˜ê³ , LangGraph ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì„ ì§€ì›í•©ë‹ˆë‹¤.

- `SupervisorAgent`, `SpecialistAgent` êµ¬í˜„
- `AgentRegistry`, `AgentRouter` êµ¬í˜„
- `DelegateToSpecialist` ìœ„ì„ ë„êµ¬ êµ¬í˜„
- `Config`ì— `agent_mode` ì„¤ì • ì¶”ê°€
- LangGraph `StateGraph` ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ (ì„ íƒì )
- LLM í‰ê°€ í…ŒìŠ¤íŠ¸ (ë‹¨ì¼ vs ë©€í‹° ì—ì´ì „íŠ¸ ë¹„êµ)

### Phase 3: ê³ ê¸‰ ê¸°ëŠ¥ (10-12ì£¼)

**ëª©í‘œ**: êµ¬ì¡°í™”ëœ ëŸ°ë¶ ì‹œìŠ¤í…œ, ì‹¤í–‰ ì—”ì§„, ë¶„ì„ ê¸°ëŠ¥ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

- `runbook/v1` YAML ìŠ¤í‚¤ë§ˆ ì •ì˜
- `RunbookExecutor` ì‹¤í–‰ ì—”ì§„ êµ¬í˜„
- `RunbookTracker`, `RunbookAnalytics` êµ¬í˜„
- ë‹¤ë‹¨ê³„ ëŸ°ë¶ ì„ íƒ ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„
- Remediation ê¸°ëŠ¥ (ê¸°ì¡´ restricted ë„êµ¬ ìŠ¹ì¸ íë¦„ í™œìš©)
- ëŸ°ë¶ A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬

### Gantt ì°¨íŠ¸

```mermaid
gantt
    title HolmesGPT ë”¥ ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜ êµ¬í˜„ ë¡œë“œë§µ
    dateFormat  YYYY-MM-DD
    axisFormat  %mì›”

    section Phase 1 - ê¸°ë°˜ êµ¬ì¶•
    BaseAgent ì¶”ìƒ í´ë˜ìŠ¤           :p1_1, 2025-09-01, 2w
    InvestigationState ëª¨ë¸         :p1_2, after p1_1, 1w
    LangChain ì–´ëŒ‘í„° êµ¬í˜„           :p1_3, after p1_1, 3w
    ì„ íƒì  ì˜ì¡´ì„± ì„¤ì •               :p1_4, after p1_3, 1w
    ë„ë©”ì¸ë³„ ë„êµ¬ì…‹ ë¶„ë¥˜             :p1_5, 2025-09-01, 2w
    Phase 1 í…ŒìŠ¤íŠ¸                  :p1_6, after p1_4, 2w

    section Phase 2 - ë©€í‹° ì—ì´ì „íŠ¸
    SupervisorAgent êµ¬í˜„            :p2_1, after p1_6, 3w
    SpecialistAgent êµ¬í˜„            :p2_2, after p1_6, 2w
    AgentRegistry / Router          :p2_3, after p2_2, 2w
    DelegateToSpecialist ë„êµ¬       :p2_4, after p2_3, 1w
    LangGraph ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜         :p2_5, after p2_1, 2w
    LLM í‰ê°€ í…ŒìŠ¤íŠ¸                 :p2_6, after p2_4, 2w

    section Phase 3 - ê³ ê¸‰ ê¸°ëŠ¥
    runbook/v1 ìŠ¤í‚¤ë§ˆ ì •ì˜           :p3_1, after p2_6, 1w
    RunbookExecutor êµ¬í˜„            :p3_2, after p3_1, 3w
    RunbookTracker / Analytics      :p3_3, after p3_2, 2w
    ë‹¤ë‹¨ê³„ ëŸ°ë¶ ì„ íƒ                 :p3_4, after p3_1, 2w
    Remediation ê¸°ëŠ¥                :p3_5, after p3_2, 2w
    A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬            :p3_6, after p3_3, 2w
```

---

## 6. ìœ„í—˜ í‰ê°€ ë° ì™„í™” ì „ëµ

### ìœ„í—˜ ë§¤íŠ¸ë¦­ìŠ¤

| ìœ„í—˜ ìš”ì†Œ | ë°œìƒ í™•ë¥  | ì˜í–¥ë„ | ë“±ê¸‰ | ì„¤ëª… |
|-----------|----------|--------|------|------|
| LLM ë¹„ìš© ì¦ê°€ | ë†’ìŒ | ì¤‘ê°„ | ğŸŸ¡ | ë©€í‹° ì—ì´ì „íŠ¸ ì‚¬ìš© ì‹œ Supervisor + Specialist í˜¸ì¶œë¡œ API ë¹„ìš© ì¦ê°€ |
| ì¡°ì‚¬ ì •í™•ë„ ì €í•˜ | ì¤‘ê°„ | ë†’ìŒ | ğŸŸ¡ | ì—ì´ì „íŠ¸ ê°„ ìœ„ì„ ê³¼ì •ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ì†ì‹¤ ê°€ëŠ¥ |
| ê¸°ì¡´ í˜¸í™˜ì„± íŒŒê´´ | ë‚®ìŒ | ë†’ìŒ | ğŸŸ¢ | ë‹¨ì¼ ì—ì´ì „íŠ¸ ëª¨ë“œë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ìœ ì§€í•˜ì—¬ ìœ„í—˜ ìµœì†Œí™” |
| LangChain ì˜ì¡´ì„± ì¶©ëŒ | ì¤‘ê°„ | ì¤‘ê°„ | ğŸŸ¡ | LangChain ë²„ì „ ì—…ë°ì´íŠ¸ì— ë”°ë¥¸ API ë³€ê²½ ê°€ëŠ¥ |
| ëŸ°ë¶ ì‹¤í–‰ ì•ˆì „ì„± | ë‚®ìŒ | ë†’ìŒ | ğŸŸ¢ | ê¸°ì¡´ restricted/approval ë©”ì»¤ë‹ˆì¦˜ì„ ì¬í™œìš©í•˜ì—¬ ìœ„í—˜ ìµœì†Œí™” |
| ë””ë²„ê¹… ë³µì¡ë„ ì¦ê°€ | ë†’ìŒ | ì¤‘ê°„ | ğŸŸ¡ | ë©€í‹° ì—ì´ì „íŠ¸ ê°„ ìƒíƒœ ì „íŒŒ ì¶”ì ì´ ì–´ë ¤ì›Œì§ˆ ìˆ˜ ìˆìŒ |

### ìœ„í—˜ë³„ ì™„í™” ì „ëµ

| ìœ„í—˜ ìš”ì†Œ | ì™„í™” ì „ëµ |
|-----------|----------|
| LLM ë¹„ìš© ì¦ê°€ | Specialistì—ê²Œ ë„ë©”ì¸ë³„ ë„êµ¬ë§Œ ì „ë‹¬í•˜ì—¬ í† í° ì ˆì•½. ë‹¨ìˆœ ì§ˆë¬¸ì€ ë‹¨ì¼ ì—ì´ì „íŠ¸ë¡œ ìë™ í´ë°±. ë¹„ìš© ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì¶• |
| ì¡°ì‚¬ ì •í™•ë„ ì €í•˜ | InvestigationStateë¥¼ í†µí•œ êµ¬ì¡°í™”ëœ ìƒíƒœ ê³µìœ . LLM í‰ê°€ í…ŒìŠ¤íŠ¸ë¡œ ë‹¨ì¼/ë©€í‹° ì—ì´ì „íŠ¸ ì •í™•ë„ ë¹„êµ. ì •í™•ë„ê°€ ë‚®ìœ¼ë©´ ìë™ ë‹¨ì¼ ì—ì´ì „íŠ¸ í´ë°± |
| ê¸°ì¡´ í˜¸í™˜ì„± íŒŒê´´ | `agent_mode: single`ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ìœ ì§€. ë©€í‹° ì—ì´ì „íŠ¸ëŠ” opt-in. ëª¨ë“  ê¸°ì¡´ í…ŒìŠ¤íŠ¸ê°€ ë‹¨ì¼ ì—ì´ì „íŠ¸ ëª¨ë“œì—ì„œ í†µê³¼í•´ì•¼ ë¨¸ì§€ í—ˆìš© |
| LangChain ì˜ì¡´ì„± ì¶©ëŒ | ì„ íƒì  ì˜ì¡´ì„±(extras)ìœ¼ë¡œ ê´€ë¦¬. ì–´ëŒ‘í„° ê³„ì¸µì—ì„œ ë²„ì „ í˜¸í™˜ì„± ì¶”ìƒí™”. LangChain ì—†ì´ë„ ì „ì²´ ê¸°ëŠ¥ ì •ìƒ ì‘ë™ ë³´ì¥ |
| ëŸ°ë¶ ì‹¤í–‰ ì•ˆì „ì„± | ê¸°ì¡´ `Tool.restricted` + `ApprovalRequirement` ë©”ì»¤ë‹ˆì¦˜ ì¬í™œìš©. ë³µêµ¬ ì‘ì—…ì€ í•­ìƒ ì‚¬ìš©ì ìŠ¹ì¸ í•„ìš”. ëŸ°ë¶ ì‹¤í–‰ ë¡œê·¸ ìë™ ê¸°ë¡ |
| ë””ë²„ê¹… ë³µì¡ë„ ì¦ê°€ | InvestigationStateì— ì „ì²´ ì‹¤í–‰ ì´ë ¥ ê¸°ë¡. ëŸ°ë¶ ì‹¤í–‰ ì¶”ì (RunbookTracker) ë„ì…. ì—ì´ì „íŠ¸ë³„ ìƒì„¸ ë¡œê¹… |

### Phaseë³„ ë¡¤ë°± ì „ëµ

| Phase | ë¡¤ë°± ì¡°ê±´ | ë¡¤ë°± ë°©ë²• |
|-------|----------|----------|
| Phase 1 | ì–´ëŒ‘í„° ê³„ì¸µì´ ê¸°ì¡´ ë„êµ¬ ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¤ëŠ” ê²½ìš° | ì–´ëŒ‘í„° ì½”ë“œë¥¼ ì œê±°í•˜ê³  ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€. ì„ íƒì  ì˜ì¡´ì„±ì´ë¯€ë¡œ í•µì‹¬ ê¸°ëŠ¥ì— ì˜í–¥ ì—†ìŒ |
| Phase 2 | ë©€í‹° ì—ì´ì „íŠ¸ ì •í™•ë„ê°€ ë‹¨ì¼ ì—ì´ì „íŠ¸ ëŒ€ë¹„ 10% ì´ìƒ ë‚®ì€ ê²½ìš° | `agent_mode: single`ì„ ìœ ì¼í•œ ì˜µì…˜ìœ¼ë¡œ ë³µì›. Supervisor/Specialist ì½”ë“œëŠ” ë¹„í™œì„±í™”í•˜ë˜ ì‚­ì œí•˜ì§€ ì•ŠìŒ |
| Phase 3 | êµ¬ì¡°í™”ëœ ëŸ°ë¶ ì‹¤í–‰ì´ ê¸°ì¡´ ë§ˆí¬ë‹¤ìš´ ëŸ°ë¶ ëŒ€ë¹„ ì„±ê³¼ê°€ ë‚®ì€ ê²½ìš° | `runbook/v1` í˜•ì‹ íŒŒì„œë¥¼ ë¹„í™œì„±í™”í•˜ê³  ë§ˆí¬ë‹¤ìš´ í´ë°± ì‚¬ìš©. RunbookExecutorëŠ” ë¹„í™œì„±í™”í•˜ë˜ ì½”ë“œ ìœ ì§€ |
